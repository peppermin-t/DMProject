{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 引入需要的包"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba874b97-7162-4675-9c49-544e23b1d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers import BertPreTrainedModel, BertTokenizer, BertConfig, BertModel, AutoConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 加载数据"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30e7c210-ac6b-4b60-978b-425e1c1111e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42790/42790 [00:00<00:00, 309997.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      id                                            content  \\\n",
      "0          1171_0001_A_1          天空下着暴雨，o2正在给c1穿雨衣，他自己却只穿着单薄的军装，完全暴露在大雨之中。   \n",
      "1          1171_0001_A_2          天空下着暴雨，o2正在给c1穿雨衣，他自己却只穿着单薄的军装，完全暴露在大雨之中。   \n",
      "2          1171_0001_A_3                       o2一手拿着一个行李，一路小跑着把c1带到了文工团门口。   \n",
      "3          1171_0001_A_4                       o2一手拿着一个行李，一路小跑着把c1带到了文工团门口。   \n",
      "4          1171_0001_A_5  o2停下来接过c1手里的行李：你妈妈交待我了，等领了军装一定要照张相寄回去，让街坊邻居都知道...   \n",
      "...                  ...                                                ...   \n",
      "42785  34949_0180_A_1450                                x2的未婚夫手捧大束玫瑰花在寻找x2。   \n",
      "42786  34949_0180_A_1458                f1看看表：过了五分钟。他从口袋里掏出药盒，倒出药：一片，两片，半片。   \n",
      "42787  34949_0180_A_1459  b1转脸朝前走。突然站住了，他看见自己的前妻，c1领着女儿g3站在前面。他过去，三人紧紧拥在...   \n",
      "42788  34949_0180_A_1460  b1转脸朝前走。突然站住了，他看见自己的前妻，c1领着女儿g3站在前面。他过去，三人紧紧拥在...   \n",
      "42789  34949_0180_A_1463  b1转脸朝前走。突然站住了，他看见自己的前妻，c1领着女儿g3站在前面。他过去，三人紧紧拥在...   \n",
      "\n",
      "      character     emotions  \n",
      "0            o2  0,0,0,0,0,0  \n",
      "1            c1  0,0,0,0,0,0  \n",
      "2            o2  0,0,0,0,0,0  \n",
      "3            c1  0,0,0,0,0,0  \n",
      "4            o2  0,0,0,0,0,0  \n",
      "...         ...          ...  \n",
      "42785        x2  0,0,0,0,0,0  \n",
      "42786        f1  0,3,0,0,0,0  \n",
      "42787        b1  2,3,0,0,0,0  \n",
      "42788        c1  2,3,0,0,0,0  \n",
      "42789        n1  0,0,0,0,0,0  \n",
      "\n",
      "[36782 rows x 4 columns]\n",
      "                      id                   content character\n",
      "0        34170_0002_A_12       穿着背心的b1醒来，看看手机，三点了。        b1\n",
      "1        34170_0002_A_14                   b1走出卧室。        b1\n",
      "2        34170_0003_A_16            b1拿着手机，点开计时功能。        b1\n",
      "3        34170_0003_A_17  b1站在淋浴头下面，水从b1的头和脸上冲刷而过。        b1\n",
      "4        34170_0003_A_18                   b1摈着呼吸。        b1\n",
      "...                  ...                       ...       ...\n",
      "21371  34122_0130_A_1184               w2：正版药进医保了。        w2\n",
      "21372  34122_0130_A_1185                   u2：那挺好。        u2\n",
      "21373  34122_0130_A_1186      w2：你还是接着卖你的壮阳药吧，适合你。        w2\n",
      "21374  34122_0130_A_1187           w2开着车，带着u2驶向远方。        u2\n",
      "21375  34122_0130_A_1188           w2开着车，带着u2驶向远方。        w2\n",
      "\n",
      "[21376 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "with open('data/train_dataset_v2.tsv', 'r', encoding='utf-8') as handler:\n",
    "    lines = handler.read().split('\\n')[1:-1]\n",
    "\n",
    "    data = list()\n",
    "    for line in tqdm(lines):\n",
    "        sp = line.split('\\t')\n",
    "        if len(sp) != 4:\n",
    "            print(\"Error: \", sp)\n",
    "            continue\n",
    "        data.append(sp)\n",
    "\n",
    "train = pd.DataFrame(data)  # 训练集\n",
    "train.columns = ['id', 'content', 'character', 'emotions']\n",
    "\n",
    "test = pd.read_csv('data/test_dataset.tsv', sep='\\t')  # 测试数据集\n",
    "submit = pd.read_csv('data/submit_example.tsv', sep='\\t')  # 提交样例数据\n",
    "train = train[train['emotions'] != '']  # 筛掉emotions列为空的行\n",
    "print(train)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a310ab7-ab8d-47dc-b0d9-60442983dc9f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train['text'] = train['content'].astype(str)  +'角色: ' + train['character'].astype(str)\n",
    "test['text'] = test['content'].astype(str) + ' 角色: ' + test['character'].astype(str)\n",
    "train['emotions'] = train['emotions'].apply(lambda x: [int(_i) for _i in x.split(',')])  # 将emotions列str转化为list\n",
    "# print(train)\n",
    "train[['love', 'joy', 'fright', 'anger', 'fear', 'sorrow']] = train['emotions'].values.tolist()\n",
    "# print(train)\n",
    "test[['love', 'joy', 'fright', 'anger', 'fear', 'sorrow']] =[0,0,0,0,0,0]\n",
    "\n",
    "train.to_csv('data/train.csv',columns=['id', 'content', 'character','text','love', 'joy', 'fright', 'anger', 'fear', 'sorrow'],\n",
    "            sep='\\t',\n",
    "            index=False)\n",
    "\n",
    "test.to_csv('data/test.csv',columns=['id', 'content', 'character','text','love', 'joy', 'fright', 'anger', 'fear', 'sorrow'],\n",
    "            sep='\\t',\n",
    "            index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "1fc0b081-505e-4bbf-8083-4e09ffba7ae0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 定义dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "target_cols=['love', 'joy', 'fright', 'anger', 'fear', 'sorrow']\n",
    "class RoleDataset(Dataset):\n",
    "    def __init__(self, tokenizer, max_len, mode='train'):\n",
    "        super(RoleDataset, self).__init__()\n",
    "        if mode == 'train':\n",
    "            self.data = pd.read_csv('data/train.csv',sep='\\t')\n",
    "        else:\n",
    "            self.data = pd.read_csv('data/test.csv',sep='\\t')\n",
    "        self.texts=self.data['text'].tolist()\n",
    "        self.labels=self.data[target_cols].to_dict('records')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text=str(self.texts[index])\n",
    "        label=self.labels[index]\n",
    "\n",
    "        encoding=self.tokenizer.encode_plus(text,\n",
    "                                            add_special_tokens=True,\n",
    "                                            max_length=self.max_len,\n",
    "                                            return_token_type_ids=True,\n",
    "                                            pad_to_max_length=True,\n",
    "                                            return_attention_mask=True,\n",
    "                                            return_tensors='pt',)\n",
    "        # print(encoding)\n",
    "\n",
    "        sample = {\n",
    "            'texts': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "        for label_col in target_cols:\n",
    "            sample[label_col] = torch.tensor(label[label_col]/3.0, dtype=torch.float)\n",
    "        print(sample)\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "ef831ad0-e7e4-4929-9640-98a4fa3efc6b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 创建dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def create_dataloader(dataset, batch_size, mode='train'):\n",
    "    shuffle = True if mode == 'train' else False\n",
    "\n",
    "    if mode == 'train':\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return data_loader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "246696b6-fe97-4a03-870a-ea815f7d3bc3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 加载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# roberta\n",
    "PRE_TRAINED_MODEL_NAME='hfl/chinese-roberta-wwm-ext'  # 'hfl/chinese-roberta-wwm-ext'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "base_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)  # 加载预训练模型\n",
    "# model = ppnlp.transformers.BertForSequenceClassification.from_pretrained(MODEL_NAME, num_classes=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "5b209dd1-7a66-4568-8070-637dfd904ff0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def init_params(module_lst):\n",
    "    for module in module_lst:\n",
    "        for param in module.parameters():\n",
    "            if param.dim() > 1:\n",
    "                torch.nn.init.xavier_uniform_(param)  # ?\n",
    "    return\n",
    "\n",
    "class IQIYModelLite(nn.Module):\n",
    "    def __init__(self, n_classes, model_name):\n",
    "        super(IQIYModelLite, self).__init__()\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.update({\"output_hidden_states\": True,\n",
    "                       \"hidden_dropout_prob\": 0.0,\n",
    "                       \"layer_norm_eps\": 1e-7})\n",
    "\n",
    "        self.base = BertModel.from_pretrained(model_name, config=config)\n",
    "        # print(self.base)\n",
    "\n",
    "        dim = 1024 if 'large' in model_name else 768\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(dim, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        # self.attention = AttentionHead(h_size=dim, hidden_dim=512, w_drop=0.0, v_drop=0.0)\n",
    "\n",
    "        self.out_love = nn.Sequential(\n",
    "            nn.Linear(dim, n_classes)\n",
    "        )\n",
    "        self.out_joy = nn.Sequential(\n",
    "            nn.Linear(dim, n_classes)\n",
    "        )\n",
    "        self.out_fright = nn.Sequential(\n",
    "            nn.Linear(dim, n_classes)\n",
    "        )\n",
    "        self.out_anger = nn.Sequential(\n",
    "            nn.Linear(dim, n_classes)\n",
    "        )\n",
    "        self.out_fear = nn.Sequential(\n",
    "            nn.Linear(dim, n_classes)\n",
    "        )\n",
    "        self.out_sorrow = nn.Sequential(\n",
    "            nn.Linear(dim, n_classes)\n",
    "        )\n",
    "\n",
    "        init_params([self.out_love, self.out_joy, self.out_fright, self.out_anger,\n",
    "                     self.out_fear,  self.out_sorrow, self.attention])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        print(\"input_ids:\")\n",
    "        print(input_ids.size())  # [128]\n",
    "        print(input_ids)\n",
    "        roberta_output = self.base(input_ids=input_ids,\n",
    "                                   attention_mask=attention_mask)\n",
    "        # print(\"roberta_output:\")\n",
    "        # print(roberta_output.hidden_states)\n",
    "        last_layer_hidden_states = roberta_output.hidden_states[-1]\n",
    "        # print(\"last_layer_hidden_states:\")\n",
    "        # print(last_layer_hidden_states.size())  # [128, 768]\n",
    "        # print(last_layer_hidden_states)\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "        # print(\"weights:\")\n",
    "        # print(weights.size())  # [128, 1]\n",
    "        # print(weights)\n",
    "        print((weights*last_layer_hidden_states).size())  # [128, 768]\n",
    "        context_vector = torch.sum(weights*last_layer_hidden_states, dim=1)\n",
    "        print(\"context_vector:\")\n",
    "        print(context_vector.size())  # [768]\n",
    "        print(context_vector)\n",
    "\n",
    "        love = self.out_love(context_vector)\n",
    "        joy = self.out_joy(context_vector)\n",
    "        fright = self.out_fright(context_vector)\n",
    "        anger = self.out_anger(context_vector)\n",
    "        fear = self.out_fear(context_vector)\n",
    "        sorrow = self.out_sorrow(context_vector)\n",
    "        print(love.size())  # [1]\n",
    "\n",
    "        return {\n",
    "            'love': love, 'joy': joy, 'fright': fright,\n",
    "            'anger': anger, 'fear': fear, 'sorrow': sorrow,\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "6d7bd256-c2c0-43b1-9d1b-04304888d919",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=2\n",
    "weight_decay=0.0\n",
    "data_path='data'\n",
    "warmup_proportion=0.0\n",
    "batch_size=16\n",
    "lr = 1e-5\n",
    "max_len = 128\n",
    "\n",
    "warm_up_ratio = 0\n",
    "\n",
    "trainset = RoleDataset(tokenizer, max_len, mode='train')\n",
    "train_loader = create_dataloader(trainset, batch_size, mode='train')\n",
    "\n",
    "valset = RoleDataset(tokenizer, max_len, mode='test')\n",
    "valid_loader = create_dataloader(valset, batch_size, mode='test')\n",
    "\n",
    "model = IQIYModelLite(n_classes=1, model_name=PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "if torch.cuda.device_count()>1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay) # correct_bias=False,\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=warm_up_ratio*total_steps,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "a9b82791-d75d-4a96-8a6c-7f5eec626c48",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "D:\\Softwares\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'texts': 's2围了过来，射杀了几名打算反抗的人，然后搜寻着他们的尸体。角色: s2', 'input_ids': tensor([  101, 11302,  1741,   749,  6814,  3341,  8024,  2198,  3324,   749,\n",
      "         1126,  1399,  2802,  5050,  1353,  2834,  4638,   782,  8024,  4197,\n",
      "         1400,  3017,  2192,  4708,   800,   812,  4638,  2221,   860,   511,\n",
      "         6235,  5682,   131, 11302,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'love': tensor(0.), 'joy': tensor(0.), 'fright': tensor(0.), 'anger': tensor(0.), 'fear': tensor(0.), 'sorrow': tensor(0.)}\n",
      "{'texts': 'a1:手下败将!角色: a1', 'input_ids': tensor([ 101, 9454,  131, 2797,  678, 6571, 2199,  106, 6235, 5682,  131, 9454,\n",
      "         102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'love': tensor(0.), 'joy': tensor(0.), 'fright': tensor(0.), 'anger': tensor(0.6667), 'fear': tensor(0.), 'sorrow': tensor(0.)}\n",
      "{'texts': 'c1（VO）：找到那个供货商了吗？角色: c1', 'input_ids': tensor([  101, 10905,  8020,   164,  8167,  8021,  8038,  2823,  1168,  6929,\n",
      "          702,   897,  6573,  1555,   749,  1408,  8043,  6235,  5682,   131,\n",
      "        10905,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'love': tensor(0.), 'joy': tensor(0.), 'fright': tensor(0.), 'anger': tensor(0.), 'fear': tensor(0.), 'sorrow': tensor(0.)}\n",
      "{'texts': 'c1：s2……s2……快起来……竟然在屁股…角色: c1', 'input_ids': tensor([  101, 10905,  8038, 11302,   100,   100, 11302,   100,   100,  2571,\n",
      "         6629,  3341,   100,   100,  4994,  4197,  1762,  2230,  5500,   100,\n",
      "         6235,  5682,   131, 10905,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'love': tensor(0.), 'joy': tensor(0.), 'fright': tensor(0.), 'anger': tensor(0.), 'fear': tensor(0.), 'sorrow': tensor(0.3333)}\n",
      "{'texts': 's2一上楼，看到自己房间里收拾过了，不由得楞了一下，然后把书包往套廊的桌子上一扔，生气地叫起来。角色: s2', 'input_ids': tensor([  101, 11302,   671,   677,  3517,  8024,  4692,  1168,  5632,  2346,\n",
      "         2791,  7313,  7027,  3119,  2896,  6814,   749,  8024,   679,  4507,\n",
      "         2533,  3506,   749,   671,   678,  8024,  4197,  1400,  2828,   741,\n",
      "         1259,  2518,  1947,  2443,  4638,  3430,  2094,   677,   671,  2803,\n",
      "         8024,  4495,  3698,  1765,  1373,  6629,  3341,   511,  6235,  5682,\n",
      "          131, 11302,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'love': tensor(0.), 'joy': tensor(0.), 'fright': tensor(0.3333), 'anger': tensor(0.6667), 'fear': tensor(0.), 'sorrow': tensor(0.)}\n",
      "{'texts': 'm2：嗯。角色: m2', 'input_ids': tensor([ 101, 9567, 8038, 1638,  511, 6235, 5682,  131, 9567,  102,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'love': tensor(0.), 'joy': tensor(0.), 'fright': tensor(0.), 'anger': tensor(0.3333), 'fear': tensor(0.), 'sorrow': tensor(0.3333)}\n",
      "{'texts': 'g1看着直播，正在和人通话。角色: g1', 'input_ids': tensor([  101, 11528,  4692,  4708,  4684,  3064,  8024,  3633,  1762,  1469,\n",
      "          782,  6858,  6413,   511,  6235,  5682,   131, 11528,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'love': tensor(0.), 'joy': tensor(0.), 'fright': tensor(0.), 'anger': tensor(0.), 'fear': tensor(0.), 'sorrow': tensor(0.)}\n",
      "{'texts': 'o2：才五十？角色: o2', 'input_ids': tensor([ 101,  157, 8144, 8038, 2798,  758, 1282, 8043, 6235, 5682,  131,  157,\n",
      "        8144,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'love': tensor(0.), 'joy': tensor(0.), 'fright': tensor(0.3333), 'anger': tensor(0.), 'fear': tensor(0.), 'sorrow': tensor(0.)}\n",
      "{'texts': 's1A：q2踩的是82防步兵地雷，爆炸方向180度，扫描面积30立方米，杀伤力98%，地雷内部装有一百颗钢珠。角色: s1', 'input_ids': tensor([  101, 10640,  8139,  8038,  9898,  6678,  4638,  3221,  8460,  7344,\n",
      "         3635,  1070,  1765,  7440,  8024,  4255,  4156,  3175,  1403,  8420,\n",
      "         2428,  8024,  2812,  2989,  7481,  4916,  8114,  4989,  3175,  5101,\n",
      "         8024,  3324,   839,  1213,  8327,   110,  8024,  1765,  7440,  1079,\n",
      "         6956,  6163,  3300,   671,  4636,  7578,  7167,  4403,   511,  6235,\n",
      "         5682,   131, 10640,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'love': tensor(0.), 'joy': tensor(0.), 'fright': tensor(0.), 'anger': tensor(0.), 'fear': tensor(0.), 'sorrow': tensor(0.)}\n",
      "{'texts': 'f2b1在g2的带领下来到酒吧，f2和i2从厕所出来，i2扶着f2胳膊，f2捂着肚子。角色: g2', 'input_ids': tensor([  101,  9675,  8204,  8148,  1762, 12179,  4638,  2372,  7566,   678,\n",
      "         3341,  1168,  6983,  1416,  8024,  9675,  1469,   151,  8144,   794,\n",
      "         1329,  2792,  1139,  3341,  8024,   151,  8144,  2820,  4708,  9675,\n",
      "         5538,  5600,  8024,  9675,  2926,  4708,  5496,  2094,   511,  6235,\n",
      "         5682,   131, 12179,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'love': tensor(0.), 'joy': tensor(0.), 'fright': tensor(0.), 'anger': tensor(0.), 'fear': tensor(0.), 'sorrow': tensor(0.6667)}\n",
      "{'texts': 'q2：你想知道事情的真相？角色: q2', 'input_ids': tensor([ 101, 9898, 8038,  872, 2682, 4761, 6887,  752, 2658, 4638, 4696, 4685,\n",
      "        8043, 6235, 5682,  131, 9898,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'love': tensor(0.), 'joy': tensor(0.), 'fright': tensor(0.), 'anger': tensor(0.), 'fear': tensor(0.), 'sorrow': tensor(0.)}\n",
      "{'texts': 'i2：这么简单？角色: i2', 'input_ids': tensor([ 101,  151, 8144, 8038, 6821,  720, 5042, 1296, 8043, 6235, 5682,  131,\n",
      "         151, 8144,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'love': tensor(0.), 'joy': tensor(0.), 'fright': tensor(0.), 'anger': tensor(0.), 'fear': tensor(0.), 'sorrow': tensor(0.)}\n",
      "{'texts': 'o2：妈啦，成绩好就可以瞧不起人啊？那妳继续瞧不起我好了，我没差。角色: o2', 'input_ids': tensor([ 101,  157, 8144, 8038, 1968, 1568, 8024, 2768, 5327, 1962, 2218, 1377,\n",
      "         809, 4743,  679, 6629,  782, 1557, 8043, 6929, 1986, 5326, 5330, 4743,\n",
      "         679, 6629, 2769, 1962,  749, 8024, 2769, 3766, 2345,  511, 6235, 5682,\n",
      "         131,  157, 8144,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'love': tensor(0.), 'joy': tensor(0.), 'fright': tensor(0.), 'anger': tensor(0.), 'fear': tensor(0.), 'sorrow': tensor(0.)}\n",
      "{'texts': 'e1：有什么好奇怪的呢？角色: e1', 'input_ids': tensor([  101, 12298,  8038,  3300,   784,   720,  1962,  1936,  2597,  4638,\n",
      "         1450,  8043,  6235,  5682,   131, 12298,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'love': tensor(0.), 'joy': tensor(0.), 'fright': tensor(0.), 'anger': tensor(0.3333), 'fear': tensor(0.), 'sorrow': tensor(0.)}\n",
      "{'texts': 'i2：晚上的音乐才艺大赛，我一定要去。角色: i2', 'input_ids': tensor([ 101,  151, 8144, 8038, 3241,  677, 4638, 7509,  727, 2798, 5686, 1920,\n",
      "        6612, 8024, 2769,  671, 2137, 6206, 1343,  511, 6235, 5682,  131,  151,\n",
      "        8144,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'love': tensor(0.), 'joy': tensor(0.), 'fright': tensor(0.), 'anger': tensor(0.), 'fear': tensor(0.), 'sorrow': tensor(0.)}\n",
      "{'texts': 'f1：我不想吃饺子。角色: f1', 'input_ids': tensor([ 101, 9080, 8038, 2769,  679, 2682, 1391, 7659, 2094,  511, 6235, 5682,\n",
      "         131, 9080,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'love': tensor(0.), 'joy': tensor(0.), 'fright': tensor(0.), 'anger': tensor(0.), 'fear': tensor(0.), 'sorrow': tensor(0.)}\n",
      "tensor([[  101, 11302,  1741,  ...,     0,     0,     0],\n",
      "        [  101,  9454,   131,  ...,     0,     0,     0],\n",
      "        [  101, 10905,  8020,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 12298,  8038,  ...,     0,     0,     0],\n",
      "        [  101,   151,  8144,  ...,     0,     0,     0],\n",
      "        [  101,  9080,  8038,  ...,     0,     0,     0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "input_ids:\n",
      "torch.Size([16, 128])\n",
      "tensor([[  101, 11302,  1741,  ...,     0,     0,     0],\n",
      "        [  101,  9454,   131,  ...,     0,     0,     0],\n",
      "        [  101, 10905,  8020,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 12298,  8038,  ...,     0,     0,     0],\n",
      "        [  101,   151,  8144,  ...,     0,     0,     0],\n",
      "        [  101,  9080,  8038,  ...,     0,     0,     0]])\n",
      "torch.Size([16, 128, 768])\n",
      "context_vector:\n",
      "torch.Size([16, 768])\n",
      "tensor([[ 0.0860, -0.1192,  0.1515,  ..., -0.0826, -0.1308, -0.2866],\n",
      "        [ 0.2072,  0.0408,  0.2626,  ..., -0.1793, -0.3011, -0.2575],\n",
      "        [ 0.1024,  0.1114,  0.3610,  ..., -0.1103,  0.0205, -0.2548],\n",
      "        ...,\n",
      "        [ 0.1614,  0.0903,  0.2605,  ..., -0.1355, -0.1385, -0.4162],\n",
      "        [ 0.3099,  0.1281,  0.2603,  ..., -0.0069, -0.2009, -0.3804],\n",
      "        [ 0.3469,  0.1960,  0.0835,  ...,  0.0870, -0.2529, -0.2671]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "torch.Size([16, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\CHENYI~1\\AppData\\Local\\Temp/ipykernel_30388/290952970.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     40\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 41\u001B[1;33m \u001B[0mdo_train\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscheduler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     42\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Users\\CHENYI~1\\AppData\\Local\\Temp/ipykernel_30388/290952970.py\u001B[0m in \u001B[0;36mdo_train\u001B[1;34m(model, date_loader, criterion, optimizer, scheduler, metric)\u001B[0m\n\u001B[0;32m     24\u001B[0m             \u001B[0mlosses\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 26\u001B[1;33m             \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     27\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[1;31m#             nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Softwares\\Python\\Python39\\lib\\site-packages\\torch\\tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph)\u001B[0m\n\u001B[0;32m    219\u001B[0m                 \u001B[0mretain_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    220\u001B[0m                 create_graph=create_graph)\n\u001B[1;32m--> 221\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    222\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    223\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Softwares\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001B[0m\n\u001B[0;32m    128\u001B[0m         \u001B[0mretain_graph\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    129\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 130\u001B[1;33m     Variable._execution_engine.run_backward(\n\u001B[0m\u001B[0;32m    131\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    132\u001B[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def do_train(model, date_loader, criterion, optimizer, scheduler, metric=None):\n",
    "    model.train()\n",
    "    global_step = 0\n",
    "    tic_train = time.time()\n",
    "    log_steps = 100\n",
    "    for epoch in range(EPOCHS):\n",
    "        losses = []\n",
    "        for step, sample in enumerate(train_loader):  # fetch data from dataloader\n",
    "            input_ids = sample[\"input_ids\"].to(device)\n",
    "            print(input_ids)\n",
    "            attention_mask = sample[\"attention_mask\"].to(device)\n",
    "            print(attention_mask)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            loss_love = criterion(outputs['love'], sample['love'].view(-1, 1).to(device))\n",
    "            loss_joy = criterion(outputs['joy'], sample['joy'].view(-1, 1).to(device))\n",
    "            loss_fright = criterion(outputs['fright'], sample['fright'].view(-1, 1).to(device))\n",
    "            loss_anger = criterion(outputs['anger'], sample['anger'].view(-1, 1).to(device))\n",
    "            loss_fear = criterion(outputs['fear'], sample['fear'].view(-1, 1).to(device))\n",
    "            loss_sorrow = criterion(outputs['sorrow'], sample['sorrow'].view(-1, 1).to(device))\n",
    "            loss = loss_love + loss_joy + loss_fright + loss_anger + loss_fear + loss_sorrow\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "#             nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % log_steps == 0:\n",
    "                print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, speed: %.2f step/s, lr: %.10f\"\n",
    "                      % (global_step, epoch, step, np.mean(losses), global_step / (time.time() - tic_train),\n",
    "                         float(scheduler.get_last_lr()[0])))\n",
    "\n",
    "\n",
    "do_train(model, train_loader, criterion, optimizer, scheduler)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "272d87a9-c527-48f6-ab22-3ad93ca95d36",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_pred = defaultdict(list)\n",
    "for step, batch in tqdm(enumerate(valid_loader)):\n",
    "    b_input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        logists = model(input_ids=b_input_ids, attention_mask=attention_mask)\n",
    "        for col in target_cols:\n",
    "            out2 = logists[col].sigmoid().squeeze(1)*3.0\n",
    "            test_pred[col].append(out2.cpu().numpy())\n",
    "\n",
    "    print(test_pred)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "72d440cd-768a-452b-89b2-9dbddf766b6f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "    val_loss = 0\n",
    "    test_pred = defaultdict(list)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    for  batch in tqdm(test_loader):\n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        with torch.no_grad():\n",
    "            logists = model(input_ids=b_input_ids, attention_mask=attention_mask)\n",
    "            for col in target_cols:\n",
    "                out2 = logists[col].sigmoid().squeeze(1)*3.0\n",
    "                test_pred[col].extend(out2.cpu().numpy().tolist())\n",
    "\n",
    "    return test_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "8bde5e31-19fa-4991-9790-fded3117899c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 加载submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submit = pd.read_csv('data/submit_example.tsv', sep='\\t')\n",
    "test_pred = predict(model, valid_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "f4bae917-4ea9-44b5-84ab-d271e7d898d8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 查看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(test_pred['love'][:10])\n",
    "print(len(test_pred['love']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "32cbe4b3-704f-4789-b4e6-37afe09380cf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 预测结果与输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "label_preds = []\n",
    "for col in target_cols:\n",
    "    preds = test_pred[col]\n",
    "    label_preds.append(preds)\n",
    "print(len(label_preds[0]))\n",
    "sub = submit.copy()\n",
    "sub['emotion'] = np.stack(label_preds, axis=1).tolist()\n",
    "sub['emotion'] = sub['emotion'].apply(lambda x: ','.join([str(i) for i in x]))\n",
    "sub.to_csv('baseline_{}.tsv'.format(PRE_TRAINED_MODEL_NAME.split('/')[-1]), sep='\\t', index=False)\n",
    "sub.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d5417e-3d6a-48d2-953b-128ccf30f202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}